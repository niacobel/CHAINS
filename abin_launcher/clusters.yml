# CECI Clusters relevant information, see http://www.ceci-hpc.be/clusters.html for more details 
# This is a YAML file, see https://yaml.org/ for more information

#! Keep in mind the definition of your scaling function when assigning values to the scale_limit keys

# ========================================================= #
#                        Shared Keys                        #
# ========================================================= #

# Those keys are shared between the clusters, they will be used for information for which the cluster doesn't matter

shared: 
  profiles:
    gaussian:
      rendering_function: &gauss_rdr chains_gaussian_render                                      
      scaling_function: &scl_gauss total_nb_elec        # Name of your scaling function (must be defined in abin_launcher/scaling_fcts.py)
      job_scales: &tiny_gauss
        label: tiny
        scale_limit: 20
        time: 0-00:10:00
        cores: 2 
        mem_per_cpu: 500 # in MB
    orca:
      rendering_function: &orca_rdr chains_orca_render                                      
      scaling_function: &scl_orca total_nb_elec         # Name of your scaling function (must be defined in abin_launcher/scaling_fcts.py)
      job_scales: &tiny_orca
        label: tiny
        scale_limit: 20
        time: 0-00:10:00
        cores: 2 
        mem_per_cpu: 500 # in MB

# ========================================================= #
#                       UMons Clusters                      #
# ========================================================= #

dragon1:
# address dragon1.umons.ac.be
  submit_command: sbatch
  profiles:
    gaussian:
      rendering_function: *gauss_rdr
      set_env:
        - module --force purge
        - module load gaussian/16_A03
      command: g16
      scaling_function: *scl_gauss
      job_scales:
        - 
          <<: *tiny_gauss 
        - 
          label: very small
          scale_limit: 200
          time: 1-00:00:00
          cores: 4
          mem_per_cpu: 750 # in MB
        - 
          label: small
          scale_limit: 500
          time: 3-00:00:00
          cores: 8
          mem_per_cpu: 750 # in MB
        - 
          label: medium
          scale_limit: 1000
          time: 5-00:00:00
          cores: 12
          mem_per_cpu: 3000 # in MB
          delay_command: --begin=now+60
        - 
          label: big
          scale_limit: 1500
          partition_name: long
          time: 10-00:00:00
          cores: 16
          mem_per_cpu: 4000 # in MB  
          delay_command: --begin=now+120
        - 
          label: very big
          scale_limit: 2500
          partition_name: long
          time: 21-00:00:00
          cores: 16
          mem_per_cpu: 6000 # in MB  
          delay_command: --begin=now+180
    orca:
      rendering_function: *orca_rdr
      set_env:
        - module --force purge
        - module load ORCA/4.2.1-gompi-2019b
      command: /usr/local/Software/.local/easybuild/software/ORCA/4.2.1-gompi-2019b/orca
      scaling_function: *scl_orca
      job_scales: 
        - 
          <<: *tiny_orca     
        - 
          label: small
          scale_limit: 500
          time: 2-00:00:00
          cores: 8
          mem_per_cpu: 750 # in MB
        - 
          label: medium
          scale_limit: 1400
          time: 5-00:00:00
          cores: 12
          mem_per_cpu: 3000 # in MB
          delay_command: --begin=now+60
        - 
          label: big
          scale_limit: 2000
          partition_name: Long
          time: 10-00:00:00
          cores: 16
          mem_per_cpu: 4000 # in MB  
          delay_command: --begin=now+120
        - 
          label: very big
          scale_limit: 2500
          partition_name: Long
          time: 21-00:00:00
          cores: 16
          mem_per_cpu: 6000 # in MB  
          delay_command: --begin=now+180

dragon2:
# address dragon2.umons.ac.be
  submit_command: sbatch
  profiles:
    gaussian:
      rendering_function: *gauss_rdr
      set_env:
        - module --force purge
        - module load gaussian/16_A03
      command: g16
      scaling_function: *scl_gauss
      job_scales:
        - 
          <<: *tiny_gauss
          partition_name: debug  
        - 
          label: small
          scale_limit: 400
          time: 5-00:00:00
          cores: 8
          mem_per_cpu: 1000 # in MB
        - 
          label: medium
          scale_limit: 1000
          partition_name: long
          time: 8-00:00:00
          cores: 12
          mem_per_cpu: 3000 # in MB
          delay_command: --begin=now+60
        - 
          label: big
          scale_limit: 1500
          partition_name: long
          time: 10-00:00:00
          cores: 16  
          mem_per_cpu: 4000 # in MB
          delay_command: --begin=now+120
        - 
          label: very big
          scale_limit: 2500
          partition_name: long
          time: 21-00:00:00
          cores: 20
          mem_per_cpu: 6000 # in MB  
          delay_command: --begin=now+180
    orca:
      rendering_function: *orca_rdr
      set_env:
        - module --force purge
        - module load releases/2019b
        - module load ORCA/4.2.1-gompi-2019b 
      command: /opt/cecisw/arch/easybuild/2019b/software/ORCA/4.2.1-gompi-2019b/orca
      scaling_function: *scl_orca
      job_scales:
        - 
          <<: *tiny_orca
          partition_name: debug  
        - 
          label: small
          scale_limit: 500
          time: 2-00:00:00
          cores: 8
          mem_per_cpu: 750 # in MB
        - 
          label: medium
          scale_limit: 1400
          time: 5-00:00:00
          cores: 12
          mem_per_cpu: 3000 # in MB
          delay_command: --begin=now+60
        - 
          label: big
          scale_limit: 2000
          partition_name: long
          time: 10-00:00:00
          cores: 16  
          mem_per_cpu: 4000 # in MB
          delay_command: --begin=now+120
        - 
          label: very big
          scale_limit: 2500
          partition_name: long
          time: 21-00:00:00
          cores: 20
          mem_per_cpu: 6000 # in MB  
          delay_command: --begin=now+180
          
# ========================================================= #
#                        UCL Clusters                       #
# ========================================================= #

lemaitre3:
# address lemaitre3.cism.ucl.ac.be
  submit_command: sbatch
  profiles:
    orca:
      rendering_function: *orca_rdr
      set_env: 
        - module --force purge
        - module load releases/2018b
        - module load ORCA/4.1.0-OpenMPI-3.1.3
      command: /opt/cecisw/arch/easybuild/2018b/software/ORCA/4.1.0-OpenMPI-3.1.3/orca
      scaling_function: *scl_orca
      job_scales:
        - 
          <<: *tiny_orca  
        - 
          label: very small
          scale_limit: 500
          time: 0-16:00:00
          cores: 8
          mem_per_cpu: 500 # in MB
        - 
          label: small
          scale_limit: 750
          time: 1-08:00:00
          cores: 12
          mem_per_cpu: 1000 # in MB
        - 
          label: medium
          scale_limit: 1000
          time: 2-00:00:00
          cores: 16
          mem_per_cpu: 2000 # in MB
          delay_command: --begin=now+60
        # - 
        #   label: big
        #   scale_limit: 1500
        #   time: 2-00:00:00
        #   cores: 20
        #   mem_per_cpu: 3000 # in MB
        #   delay_command: --begin=now+120

# ========================================================= #
#                      UNamur Clusters                      #
# ========================================================= #

hercules:
# address hercules2.ptci.unamur.be
  submit_command: sbatch
  profiles:
    qchem:
      rendering_function: chains_qchem_render           
      set_env: 
        - module --force purge
        - module load tis/2017.01
        - module load Q-Chem/5.3.0-SHMEM
        - export QCSCRATCH=${TMPDIR}
      command: qchem -nt ${SLURM_CPUS_PER_TASK}
      scaling_function: total_nb_elec
      job_scales:
        - 
          label: debug
          scale_limit: 20
          time: 0-00:05:00
          cores: 2 
          mem_per_cpu: 500 # in MB
        - 
          label: tiny
          scale_limit: 200
          time: 0-01:00:00
          cores: 4
          mem_per_cpu: 500 # in MB
        - 
          label: very small
          scale_limit: 750
          time: 1-00:00:00
          cores: 8
          mem_per_cpu: 1000 # in MB
        - 
          label: small
          scale_limit: 1200
          time: 4-00:00:00
          cores: 8
          mem_per_cpu: 1500 # in MB
        - 
          label: medium
          scale_limit: 1500
          time: 10-00:00:00
          cores: 16
          mem_per_cpu: 3000 # in MB
          delay_command: --begin=now+60
        - 
          label: big
          scale_limit: 2000
          time: 15-00:00:00
          cores: 20
          mem_per_cpu: 4000 # in MB
          delay_command: --begin=now+120
        - 
          label: very big
          scale_limit: 2500
          time: 15-00:00:00
          cores: 24
          mem_per_cpu: 6000 # in MB
          delay_command: --begin=now+180
    qchem_tzvp:
      rendering_function: chains_qchem_render           
      set_env: 
        - module --force purge
        - module load tis/2017.01
        - module load Q-Chem/5.3.0-SHMEM
        - export QCSCRATCH=${TMPDIR}
      command: qchem -nt ${SLURM_CPUS_PER_TASK}
      scaling_function: total_nb_elec
      job_scales:
        - 
          label: debug
          scale_limit: 20
          time: 0-00:05:00
          cores: 2 
          mem_per_cpu: 500 # in MB
        - 
          label: tiny
          scale_limit: 100
          time: 0-01:00:00
          cores: 4
          mem_per_cpu: 500 # in MB
        - 
          label: very small
          scale_limit: 500
          time: 1-00:00:00
          cores: 8
          mem_per_cpu: 1000 # in MB
        - 
          label: small
          scale_limit: 800
          time: 4-00:00:00
          cores: 8
          mem_per_cpu: 1500 # in MB
        - 
          label: medium
          scale_limit: 1250
          time: 10-00:00:00
          cores: 16
          mem_per_cpu: 3000 # in MB
          delay_command: --begin=now+60
        - 
          label: big
          scale_limit: 2000
          time: 15-00:00:00
          cores: 20
          mem_per_cpu: 4000 # in MB
          delay_command: --begin=now+120
        - 
          label: very big
          scale_limit: 2500
          time: 15-00:00:00
          cores: 24
          mem_per_cpu: 6000 # in MB
          delay_command: --begin=now+180
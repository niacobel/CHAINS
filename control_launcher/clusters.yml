# CECI Clusters relevant information, see http://www.ceci-hpc.be/clusters.html for more details 
# This is a YAML file, see https://yaml.org/ for more information

#! Keep in mind the definition of your scaling function when assigning values to the scale_limit keys

# ========================================================= #
#                        Shared Keys                        #
# ========================================================= #

# Those keys are shared between the clusters, they will be used for information for which the cluster doesn't matter

shared: 
  progs:
    qoctra: 
      jinja_templates: &qoctra_jinja
        parameters_file: param.nml.jinja             # The rendered file will be named param_<target>.nml, where <target> is the state label corresponding to the projector that will be used
        job_script: qoctra_job.sh.jinja                    
      job_scales: &unique_qoctra
        label: unique
        scale_limit: 100
        partition_name: default
        time: 0-15:00:00
        mem_per_cpu: 2000 # in MB

# ========================================================= #
#                        UCL Clusters                       #
# ========================================================= #

lemaitre3:
# address lemaitre3.cism.ucl.ac.be
  submit_command: sbatch
  progs:
    qoctra:
      jinja_templates: *qoctra_jinja
      set_env:
        - module --force purge
        - module load releases/2019b
        - module load OpenBLAS/0.3.7-GCC-8.3.0
      command: gfortran -lopenblas -O3 -ffast-math -funroll-loops -fwhole-program -flto -fexternal-blas -fdefault-integer-8 -m64
      job_scales:
        - <<: *unique_qoctra